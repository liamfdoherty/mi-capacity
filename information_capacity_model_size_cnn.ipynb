{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "import task_complexity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "target_transform_cifar10 = transforms.Compose([\n",
    "    lambda x: torch.tensor(x),\n",
    "    lambda x: F.one_hot(x, num_classes = 10)\n",
    "    ])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset_cifar10 = CIFAR10(root = '../example_data/', train = True, transform = transform, target_transform = target_transform_cifar10)\n",
    "trainloader_cifar10 = torch.utils.data.DataLoader(trainset_cifar10, batch_size = batch_size, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "        def __init__(self, layers = 5):\n",
    "            super().__init__()\n",
    "            self.block1 = nn.Sequential(*[nn.Conv2d(3, 16, 4, 2, 1), nn.ReLU(inplace = True), nn.BatchNorm2d(16)])\n",
    "            self.block2 = nn.Sequential(*[nn.Conv2d(16, 32, 4, 2, 1), nn.ReLU(inplace = True), nn.BatchNorm2d(32)])\n",
    "            self.block3 = nn.Sequential(*[nn.Conv2d(32, 64, 4, 2, 1), nn.ReLU(inplace = True), nn.BatchNorm2d(64)])\n",
    "            \n",
    "            self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "            self.flat = nn.Flatten()\n",
    "            self.linear = nn.Linear(64, 10)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.block1(x)\n",
    "            x = self.block2(x)\n",
    "            x = self.block3(x)\n",
    "            x = self.pool(x)\n",
    "            x = self.flat(x)\n",
    "            x = self.linear(x)\n",
    "            x = nn.Softmax()(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/liamdoherty/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.001\n",
      "[1,    21] loss: 0.022\n",
      "[1,    41] loss: 0.022\n",
      "[1,    61] loss: 0.022\n",
      "[1,    81] loss: 0.021\n",
      "[1,   101] loss: 0.021\n",
      "[1,   121] loss: 0.021\n",
      "[1,   141] loss: 0.021\n",
      "[1,   161] loss: 0.021\n",
      "[1,   181] loss: 0.021\n",
      "[1,   201] loss: 0.021\n",
      "[1,   221] loss: 0.021\n",
      "[1,   241] loss: 0.020\n",
      "[1,   261] loss: 0.020\n",
      "[1,   281] loss: 0.020\n",
      "[1,   301] loss: 0.020\n",
      "[1,   321] loss: 0.020\n",
      "[1,   341] loss: 0.020\n",
      "[1,   361] loss: 0.020\n",
      "[1,   381] loss: 0.020\n",
      "[2,     1] loss: 0.001\n",
      "[2,    21] loss: 0.020\n",
      "[2,    41] loss: 0.020\n",
      "[2,    61] loss: 0.020\n",
      "[2,    81] loss: 0.020\n",
      "[2,   101] loss: 0.020\n",
      "[2,   121] loss: 0.020\n",
      "[2,   141] loss: 0.020\n",
      "[2,   161] loss: 0.020\n",
      "[2,   181] loss: 0.020\n",
      "[2,   201] loss: 0.020\n",
      "[2,   221] loss: 0.020\n",
      "[2,   241] loss: 0.020\n",
      "[2,   261] loss: 0.020\n",
      "[2,   281] loss: 0.020\n",
      "[2,   301] loss: 0.019\n",
      "[2,   321] loss: 0.020\n",
      "[2,   341] loss: 0.020\n",
      "[2,   361] loss: 0.020\n",
      "[2,   381] loss: 0.019\n",
      "[3,     1] loss: 0.001\n",
      "[3,    21] loss: 0.019\n",
      "[3,    41] loss: 0.019\n",
      "[3,    61] loss: 0.019\n",
      "[3,    81] loss: 0.019\n",
      "[3,   101] loss: 0.019\n",
      "[3,   121] loss: 0.019\n",
      "[3,   141] loss: 0.019\n",
      "[3,   161] loss: 0.019\n",
      "[3,   181] loss: 0.019\n",
      "[3,   201] loss: 0.019\n",
      "[3,   221] loss: 0.019\n",
      "[3,   241] loss: 0.019\n",
      "[3,   261] loss: 0.019\n",
      "[3,   281] loss: 0.019\n",
      "[3,   301] loss: 0.019\n",
      "[3,   321] loss: 0.019\n",
      "[3,   341] loss: 0.019\n",
      "[3,   361] loss: 0.019\n",
      "[3,   381] loss: 0.019\n",
      "[4,     1] loss: 0.001\n",
      "[4,    21] loss: 0.019\n",
      "[4,    41] loss: 0.019\n",
      "[4,    61] loss: 0.019\n",
      "[4,    81] loss: 0.019\n",
      "[4,   101] loss: 0.019\n",
      "[4,   121] loss: 0.019\n",
      "[4,   141] loss: 0.019\n",
      "[4,   161] loss: 0.019\n",
      "[4,   181] loss: 0.019\n",
      "[4,   201] loss: 0.019\n",
      "[4,   221] loss: 0.019\n",
      "[4,   241] loss: 0.019\n",
      "[4,   261] loss: 0.019\n",
      "[4,   281] loss: 0.019\n",
      "[4,   301] loss: 0.019\n",
      "[4,   321] loss: 0.019\n",
      "[4,   341] loss: 0.019\n",
      "[4,   361] loss: 0.019\n",
      "[4,   381] loss: 0.019\n",
      "[5,     1] loss: 0.001\n",
      "[5,    21] loss: 0.019\n",
      "[5,    41] loss: 0.019\n",
      "[5,    61] loss: 0.019\n",
      "[5,    81] loss: 0.019\n",
      "[5,   101] loss: 0.018\n",
      "[5,   121] loss: 0.019\n",
      "[5,   141] loss: 0.018\n",
      "[5,   161] loss: 0.019\n",
      "[5,   181] loss: 0.019\n",
      "[5,   201] loss: 0.019\n",
      "[5,   221] loss: 0.019\n",
      "[5,   241] loss: 0.019\n",
      "[5,   261] loss: 0.019\n",
      "[5,   281] loss: 0.019\n",
      "[5,   301] loss: 0.019\n",
      "[5,   321] loss: 0.018\n",
      "[5,   341] loss: 0.019\n",
      "[5,   361] loss: 0.019\n",
      "[5,   381] loss: 0.019\n",
      "[6,     1] loss: 0.001\n",
      "[6,    21] loss: 0.018\n",
      "[6,    41] loss: 0.018\n",
      "[6,    61] loss: 0.018\n",
      "[6,    81] loss: 0.018\n",
      "[6,   101] loss: 0.018\n",
      "[6,   121] loss: 0.018\n",
      "[6,   141] loss: 0.018\n",
      "[6,   161] loss: 0.018\n",
      "[6,   181] loss: 0.018\n",
      "[6,   201] loss: 0.018\n",
      "[6,   221] loss: 0.018\n",
      "[6,   241] loss: 0.018\n",
      "[6,   261] loss: 0.018\n",
      "[6,   281] loss: 0.019\n",
      "[6,   301] loss: 0.018\n",
      "[6,   321] loss: 0.018\n",
      "[6,   341] loss: 0.018\n",
      "[6,   361] loss: 0.018\n",
      "[6,   381] loss: 0.018\n",
      "[7,     1] loss: 0.001\n",
      "[7,    21] loss: 0.018\n",
      "[7,    41] loss: 0.018\n",
      "[7,    61] loss: 0.018\n",
      "[7,    81] loss: 0.018\n",
      "[7,   101] loss: 0.018\n",
      "[7,   121] loss: 0.018\n",
      "[7,   141] loss: 0.018\n",
      "[7,   161] loss: 0.018\n",
      "[7,   181] loss: 0.018\n",
      "[7,   201] loss: 0.018\n",
      "[7,   221] loss: 0.018\n",
      "[7,   241] loss: 0.018\n",
      "[7,   261] loss: 0.018\n",
      "[7,   281] loss: 0.018\n",
      "[7,   301] loss: 0.018\n",
      "[7,   321] loss: 0.018\n",
      "[7,   341] loss: 0.018\n",
      "[7,   361] loss: 0.018\n",
      "[7,   381] loss: 0.018\n",
      "[8,     1] loss: 0.001\n",
      "[8,    21] loss: 0.018\n",
      "[8,    41] loss: 0.018\n",
      "[8,    61] loss: 0.018\n",
      "[8,    81] loss: 0.018\n",
      "[8,   101] loss: 0.018\n",
      "[8,   121] loss: 0.018\n",
      "[8,   141] loss: 0.018\n",
      "[8,   161] loss: 0.018\n",
      "[8,   181] loss: 0.018\n",
      "[8,   201] loss: 0.018\n",
      "[8,   221] loss: 0.018\n",
      "[8,   241] loss: 0.018\n",
      "[8,   261] loss: 0.018\n",
      "[8,   281] loss: 0.018\n",
      "[8,   301] loss: 0.018\n",
      "[8,   321] loss: 0.018\n",
      "[8,   341] loss: 0.018\n",
      "[8,   361] loss: 0.018\n",
      "[8,   381] loss: 0.018\n",
      "[9,     1] loss: 0.001\n",
      "[9,    21] loss: 0.018\n",
      "[9,    41] loss: 0.018\n",
      "[9,    61] loss: 0.018\n",
      "[9,    81] loss: 0.018\n",
      "[9,   101] loss: 0.018\n",
      "[9,   121] loss: 0.018\n",
      "[9,   141] loss: 0.018\n",
      "[9,   161] loss: 0.018\n",
      "[9,   181] loss: 0.018\n",
      "[9,   201] loss: 0.018\n",
      "[9,   221] loss: 0.018\n",
      "[9,   241] loss: 0.018\n",
      "[9,   261] loss: 0.018\n",
      "[9,   281] loss: 0.018\n",
      "[9,   301] loss: 0.018\n",
      "[9,   321] loss: 0.018\n",
      "[9,   341] loss: 0.018\n",
      "[9,   361] loss: 0.018\n",
      "[9,   381] loss: 0.018\n",
      "[10,     1] loss: 0.001\n",
      "[10,    21] loss: 0.018\n",
      "[10,    41] loss: 0.018\n",
      "[10,    61] loss: 0.018\n",
      "[10,    81] loss: 0.018\n",
      "[10,   101] loss: 0.018\n",
      "[10,   121] loss: 0.018\n",
      "[10,   141] loss: 0.018\n",
      "[10,   161] loss: 0.018\n",
      "[10,   181] loss: 0.018\n",
      "[10,   201] loss: 0.018\n",
      "[10,   221] loss: 0.018\n",
      "[10,   241] loss: 0.018\n",
      "[10,   261] loss: 0.018\n",
      "[10,   281] loss: 0.018\n",
      "[10,   301] loss: 0.018\n",
      "[10,   321] loss: 0.018\n",
      "[10,   341] loss: 0.018\n",
      "[10,   361] loss: 0.018\n",
      "[10,   381] loss: 0.018\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader_cifar10, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.double().to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 0:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
